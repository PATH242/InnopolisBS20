{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Information retrieval homework 1 - Crawler\n",
    "https://github.com/IUCVLab/information-retrieval/blob/main/homeworks/2023/2023S-01%20-%20Crawling.ipynb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1.1 Download and persist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "from hashlib import sha512\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.file_name_hashed = \"\"\n",
    "        self.type = \".txt\"\n",
    "    # get document via download, persist, and load\n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    # download self.url content, store it in self.content and return True in case of success\n",
    "    def download(self):\n",
    "        try:\n",
    "            response = requests.get(self.url, allow_redirects=True)\n",
    "            if response.status_code != 200:\n",
    "                print(response.status_code, response.reason, 'for', self.url)\n",
    "                return False\n",
    "            self.content = response.content\n",
    "            path = urlparse(self.url).path\n",
    "            if path:\n",
    "                self.type = os.path.splitext(path)[1]\n",
    "            print(f\"File downloaded from {self.url}\")\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # write document content to hard drive, return True in case of success\n",
    "    def persist(self):\n",
    "        try:\n",
    "            if self.file_name_hashed == \"\":\n",
    "                self.file_name_hashed =  sha512(self.url.encode()).hexdigest() + self.type\n",
    "            with open(self.file_name_hashed, 'wb') as f:\n",
    "                f.write(self.content)\n",
    "                print(f\"File saved as {self.file_name_hashed}\")\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # load content from hard drive, store it in self.content and return True in case of success\n",
    "    def load(self):\n",
    "        try:\n",
    "            with open(self.file_name_hashed, 'rb') as f:\n",
    "                self.content = f.read()\n",
    "                return True\n",
    "        except:\n",
    "            return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded from http://sprotasov.ru/data/iu.txt\n",
      "File saved as 399b014f7ddfc1e4b2721c246d3752b1159261ef9780f58f263bed08e521b28ca74c58c35806bd1871e1cb57b879a9da2e77e57dd3a17255a3f258b42377a365.txt\n",
      "Task 1.1 test Success\n"
     ]
    }
   ],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\"\n",
    "print(\"Task 1.1 test Success\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded from http://commondatastorage.googleapis.com/codeskulptor-demos/DDR_assets/Kangaroo_MusiQue_-_The_Neverwritten_Role_Playing_Game.mp3\n",
      "File saved as 26db2dd587b44363c8f6ee0acfd28dbf706e3e7a71722d7543d241e3203a58e2f3781ef242293a4c54bbffa6d46e397937ddc61f6257055ed8432d4435d9dec1.mp3\n",
      "Task 1.1 mp3 test Success\n"
     ]
    }
   ],
   "source": [
    "new_doc = Document('http://commondatastorage.googleapis.com/codeskulptor-demos/DDR_assets/Kangaroo_MusiQue_-_The_Neverwritten_Role_Playing_Game.mp3')\n",
    "new_doc.get()\n",
    "assert new_doc.load(), \"Load should return true for saved document\"\n",
    "print(\"Task 1.1 mp3 test Success\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded from https://innopolis.university/files/politicacookies.pdf?lang=ru&id=12&site=s1&template=university24&landing_mode=edit\n",
      "File saved as 183e243dcc6c242023d3c8ed65c6334d252d4d0fda2bb74220d4bddc536085610530596d7f7cf4957094a3629b5d048fdd845b5ecc7f4298878a779b40359711.pdf\n",
      "Task 1.1 pdf test Success\n"
     ]
    }
   ],
   "source": [
    "third_doc = Document('https://innopolis.university/files/politicacookies.pdf?lang=ru&id=12&site=s1&template=university24&landing_mode=edit')\n",
    "third_doc.get()\n",
    "assert new_doc.load(), \"Load should return true for saved document\"\n",
    "print(\"Task 1.1 pdf test Success\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1.2 Parse HTML"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "from bs4 import SoupStrainer\n",
    "import urllib.parse\n",
    "import httplib2\n",
    "\n",
    "class HTMLDocument(Document):\n",
    "\n",
    "    def __init__(self, url):\n",
    "        super().__init__(url)\n",
    "        self.anchors = []\n",
    "        self.images = []\n",
    "        self.text = \"\"\n",
    "    # check if element is visible\n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "    # extract text from html\n",
    "    def textFromHTML(self, body):\n",
    "        soup = BeautifulSoup(body, 'html.parser')\n",
    "        texts = soup.findAll(text=True)\n",
    "        visible_texts = filter(self.tag_visible, texts)\n",
    "        return u\"\".join(t.strip() for t in visible_texts)\n",
    "\n",
    "    # extract plain text, images and links from the document\n",
    "    def parse(self):\n",
    "        try:\n",
    "            http = httplib2.Http()\n",
    "            status, response = http.request(self.url)\n",
    "            self.text = self.textFromHTML(response)\n",
    "            for link in BeautifulSoup(response, 'html.parser', parse_only=SoupStrainer('a') ):\n",
    "                if(link.has_attr('href')):\n",
    "                    self.anchors.append((link.text, link['href']))\n",
    "            html = response.decode()\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for img in soup.find_all('img'):\n",
    "                src = img.get('src')\n",
    "                if src:\n",
    "                    src = urllib.parse.urljoin(self.url, src)\n",
    "                    self.images.append(src)\n",
    "            return True\n",
    "        except:\n",
    "            return False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded from http://sprotasov.ru\n",
      "File saved as adb3427a17aef9dbfd24527adc85614193d6831828b7303941b457a289c9463bed5ffb2f5c5dd6100f6b467e21a810908839ee5b694502cff4d7d6956b407f9c.txt\n",
      "Task 1.2 test Success\n"
     ]
    }
   ],
   "source": [
    "doc = HTMLDocument('http://sprotasov.ru')\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\"\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "print(\"Task 1.2 test Success\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1.3 Document analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "class HtmlDocumentTextData:\n",
    "    def __init__(self, url):\n",
    "        self.doc = HTMLDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "\n",
    "    # sentences parser\n",
    "    def get_sentences(self):\n",
    "        result = []\n",
    "        soup = BeautifulSoup(self.doc.content, 'html.parser')\n",
    "        tag = soup.body\n",
    "        try:\n",
    "            for s in tag.strings:\n",
    "                result.append(s.strip().lower())\n",
    "        except:\n",
    "            print(\"ERROR IN PARSING {doc.url}\")\n",
    "        return result\n",
    "\n",
    "    # return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
    "    def get_word_stats(self):\n",
    "        sentences = self.get_sentences()\n",
    "        words = []\n",
    "        for s in sentences:\n",
    "            words += re.split(' ', s)\n",
    "        for i in range( len(words) ):\n",
    "            words[i] = re.sub(r'[^\\w\\s]', '', words[i].lower())\n",
    "        words = filter(lambda x: x != '', words)\n",
    "        return Counter(words)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded from https://innopolis.university/\n",
      "File saved as 09f4ea693dd9b25ed0ccc27a46a34a00d3faec660a4b0143f63a08a631ffbe17a817de31a1448b57a318c1aa6dd71c80633263149dd438ce4a663190574121bb\n",
      "[('и', 44), ('в', 22), ('иннополис', 20), ('с', 13), ('на', 12), ('университет', 11), ('университета', 11), ('центр', 10), ('для', 9), ('образование', 8)]\n",
      "Task 1.3 test Success\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'\n",
    "print(\"Task 1.3 test Success\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Task 1.4 Crawler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    # return generator of HtmlDocumentTextData objects as per depth specified\n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        visited = {} # visited urls\n",
    "        q = Queue() # queue of urls to process\n",
    "        q.put( (0, source) )\n",
    "        while(not q.empty()):\n",
    "            try:\n",
    "                current_website = q.get()\n",
    "                if current_website[0] > depth:\n",
    "                    break\n",
    "                if current_website[1] in visited:\n",
    "                    continue\n",
    "                visited[current_website[1]] = True\n",
    "                if current_website[1][-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "                    continue\n",
    "                print(f\"Working on {current_website}\")\n",
    "                doc_data = HtmlDocumentTextData(current_website[1])\n",
    "                for childDoc in doc_data.doc.anchors:\n",
    "                    q.put((current_website[0] + 1, childDoc[1]))\n",
    "                print(\"Finished working on \", current_website)\n",
    "                yield doc_data\n",
    "            except Exception as e:\n",
    "                continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "30308 distinct words in total\n",
      "[('the', 9727), ('and', 8694), ('и', 8345), ('of', 7961), ('в', 5773), ('in', 5141), ('to', 4794), ('for', 3131), ('university', 3114), ('на', 3064), ('a', 2882), ('с', 2482), ('иннополис', 2404), ('по', 2126), ('at', 1718), ('я', 1693), ('университета', 1684), ('is', 1649), ('i', 1614), ('innopolis', 1549)]\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "\n",
    "print(\"Done\")\n",
    "print(len(counter), \"distinct words in total\")\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'иннополис'], 'иннополис sould be among most common'\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1.4 test Success\n"
     ]
    }
   ],
   "source": [
    "print(\"Task 1.4 test Success\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
